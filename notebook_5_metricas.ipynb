{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def confusion_matrix(y_actual, y_predicted, positive_label, show=False):\n",
    "    # Construye una matriz de confusión (binaria)\n",
    "    # y_actual es la secuencia de etiquetas reales\n",
    "    # y_predicted es la secuencia de etiquetas predichas por el clasificador\n",
    "    # positive_label indica cuál es la etiqueta considerada positiva.\n",
    "    n = len(y_actual)\n",
    "\n",
    "    tp = \"COMPLETAR\"  # verdaderos positivos\n",
    "    tn = \"COMPLETAR\"  # verdaderos negativos\n",
    "    fp = \"COMPLETAR\"  # falsos positivos\n",
    "    fn = \"COMPLETAR\"  # falsos negativos\n",
    "    \n",
    "    if show:\n",
    "        display(pd.DataFrame([[tp, tn], [fp, fn]], index=[\"real +\", \"real -\"], columns=[\"pred +\", \"pred -\"]))\n",
    "        \n",
    "        \n",
    "    return tp, tn, fp, fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = [\"spam\"] * 10 + [\"no-spam\"] * 978 + [\"spam\"] * 2\n",
    "y_pred = [\"spam\"] * 2 + [\"no-spam\"] * 900 + [\"spam\"] * 20 + [\"no-spam\"] * 68\n",
    "tp, tn, fp, fn = confusion_matrix(y_actual=y_actual, y_predicted=y_pred, positive_label=\"spam\")\n",
    "\n",
    "print(\"Test 1\")\n",
    "print(\"(tp, tn, fp, fn) = \", (tp, tn, fp, fn))\n",
    "assert((tp, tn, fp, fn) == (2, 958, 20, 10))\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Métricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(tp, tn, fp, fn):\n",
    "    return \"Completar\"\n",
    "\n",
    "\n",
    "def precision_score(tp, tn, fp, fn):\n",
    "    return \"Completar\"\n",
    "\n",
    "\n",
    "def recall_score(tp, tn, fp, fn):\n",
    "    return \"Completar\"\n",
    "\n",
    "\n",
    "def f_beta_score(tp, tn, fp, fn, beta):\n",
    "    prec = precision_score(tp, tn, fp, fn)\n",
    "    recl = recall_score(tp, tn, fp, fn)\n",
    "    return \"Completar\"\n",
    "\n",
    "\n",
    "def f1_score(tp, tn, fp, fn):\n",
    "    return f_beta_score(tp, tn, fp, fn, beta=1)\n",
    "\n",
    "\n",
    "def all_metrics(tp, tn, fp, fn):\n",
    "    accuracy = round(accuracy_score(tp, tn, fp, fn), 3)\n",
    "    precision = round(precision_score(tp, tn, fp, fn), 3)\n",
    "    recall = round(recall_score(tp, tn, fp, fn), 3)\n",
    "    f1 = round(f1_score(tp, tn, fp, fn), 3)\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = confusion_matrix(y_actual=y_actual, y_predicted=y_pred, positive_label=\"spam\")\n",
    "(accuracy, precision, recall, f1) = all_metrics(tp, tn, fp, fn)\n",
    "\n",
    "print(\"Test 2\")\n",
    "print(\"(accuracy, precision, recall, f1) = \", (accuracy, precision, recall, f1))\n",
    "assert((accuracy, precision, recall, f1) == (0.97, 0.091, 0.167, 0.118))\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiquetas reales\n",
    "y_actual = [\"perro\"] * 18 + [\"gato\"] * 980 + [\"perro\"] * 5\n",
    "\n",
    "# Etiquetas devueltas por \"clasificador A\"\n",
    "y_pred_1 = [\"gato\"] * 980 + [\"perro\"] * 20 + [\"gato\"] * 3\n",
    "\n",
    "# Etiquetas devueltas por \"clasificador B\"\n",
    "y_pred_2 = [\"perro\"] * 40 + [\"gato\"] * 900 + [\"perro\"] * 60 + [\"gato\"] * 3\n",
    "\n",
    "res = []\n",
    "tp, tn, fp, fn = confusion_matrix(y_actual=y_actual, y_predicted=y_pred_1, positive_label=\"gato\", show=True)\n",
    "res.append(all_metrics(tp, tn, fp, fn))\n",
    "\n",
    "tp, tn, fp, fn = confusion_matrix(y_actual=y_actual, y_predicted=y_pred_1, positive_label=\"gato\", show=True)\n",
    "res.append(all_metrics(tp, tn, fp, fn))\n",
    "\n",
    "tp, tn, fp, fn = confusion_matrix(y_actual=y_actual, y_predicted=y_pred_1, positive_label=\"perro\", show=True)\n",
    "res.append(all_metrics(tp, tn, fp, fn))\n",
    "\n",
    "tp, tn, fp, fn = confusion_matrix(y_actual=y_actual, y_predicted=y_pred_1, positive_label=\"perro\", show=True)\n",
    "res.append(all_metrics(tp, tn, fp, fn))\n",
    "\n",
    "pd.DataFrame(res, columns=[\"accuracy\", \"precision\", \"recall\", \"f1\"], index=[\"CLF A (gato)\", \"CLF B (gato)\", \"CLF A (perro)\", \"CLF B (gato)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "y_actual = [\"perro\"] * 100 + [\"gato\"] * 900 + [\"perro\"] * 80\n",
    "y_pred =   [\"perro\"] * 80 + [\"gato\"] * 800 + [\"perro\"] * 200\n",
    "\n",
    "tns_gato = []\n",
    "f1s_gato = []\n",
    "f1s_perro = []\n",
    "f1s_avg = []\n",
    "\n",
    "\n",
    "for i in range(0, 10000, 100):\n",
    "    y_actual_2 = y_actual + [\"perro\"] * i\n",
    "    y_pred_2 = y_pred + [\"perro\"] * i\n",
    "\n",
    "    tp1, tn1, fp1, fn1 = confusion_matrix(y_actual=y_actual_2, y_predicted=y_pred_2, positive_label=\"gato\")\n",
    "    tp2, tn2, fp2, fn2 = confusion_matrix(y_actual=y_actual_2, y_predicted=y_pred_2, positive_label=\"perro\")\n",
    "\n",
    "    f1_gato = f1_score(tp1, tn1, fp1, fn1)\n",
    "    f1_perro = f1_score(tp2, tn2, fp2, fn2)\n",
    "    f1_avg = (f1_gato + f1_perro) / 2\n",
    "\n",
    "    tns_gato.append(tn1)\n",
    "    f1s_gato.append(f1_gato)\n",
    "    f1s_perro.append(f1_perro)\n",
    "    f1s_avg.append(f1_avg)\n",
    "\n",
    "plt.plot(tns_gato, f1s_gato, \"*-\", label=\"f1_gato\")\n",
    "plt.plot(tns_gato, f1s_perro, \"o-\", label=\"f1_perro\")\n",
    "plt.plot(tns_gato, f1s_avg, \"x-\", label=\"f1_avg\")\n",
    "plt.xlabel(\"True Negatives (Gato)\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.ylim([0.5,1])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
